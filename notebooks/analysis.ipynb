{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import chardet\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "from langdetect import detect, LangDetectException\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden Sie Ihre Daten\n",
    "df = pd.read_csv('../data/raw/gpd_v2_20220427.csv', delimiter=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['merging_variable', 'country', 'leader', 'party', 'lr', 'president',\n",
      "       'term', 'startofterm', 'yearbegin', 'endofterm', 'yearend',\n",
      "       'speechtype', 'speechnum', 'codernum', 'rubricgrade', 'averagerubric',\n",
      "       'totalaverage', 'wb_region', 'region', 'file_contents'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# Teilen Sie die Daten in populistisch und nicht populistisch\n",
    "print(df.columns)\n",
    "\n",
    "populist_files = df[df['rubricgrade'] == 2]['merging_variable']\n",
    "non_populist_files = df[df['rubricgrade'] == 0]['merging_variable']\n",
    "\n",
    "# Verzeichnis mit Ihren Textdateien\n",
    "text_files_directory = 'C:\\\\Users\\\\furka\\\\OneDrive\\\\Documents\\\\Studium\\\\Adv.ML-Project\\\\data\\\\raw\\\\speeches_20220427\\\\'\n",
    "\n",
    "# Funktion zum Lesen der Dateiinhalte\n",
    "def read_file_contents(filename):\n",
    "    filepath = os.path.join(text_files_directory, filename)\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb') as file:\n",
    "            rawdata = file.read()\n",
    "        encoding = chardet.detect(rawdata)['encoding']\n",
    "        with open(filepath, 'r', encoding=encoding) as file:\n",
    "            return file.read().replace('\\n', '')\n",
    "\n",
    "    else:\n",
    "        return None  # oder ein leerer String '', je nachdem, was in Ihrem Fall am besten passt\n",
    "\n",
    "\n",
    "# Entferne alle Zeilen mit fehlenden Werten in 'merging_variable'\n",
    "df = df.dropna(subset=['merging_variable'])\n",
    "\n",
    "# Hinzufügen der Dateiinhalte zu Ihrem DataFrame\n",
    "df['file_contents'] = df['merging_variable'].apply(read_file_contents)\n",
    "\n",
    "\n",
    "# Speichere den bereinigten Datensatz\n",
    "df.to_csv('../data/processed/gpd_processed.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\furka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "es    120\n",
      "en     20\n",
      "ru     18\n",
      "tr     16\n",
      "mk     10\n",
      "et      8\n",
      "cs      8\n",
      "ro      8\n",
      "sk      8\n",
      "tl      8\n",
      "hr      4\n",
      "hu      4\n",
      "it      4\n",
      "fr      4\n",
      "bg      4\n",
      "uk      4\n",
      "Name: count, dtype: int64\n",
      "Top bigrams for en: [('islamic state', 60), ('brothers sisters', 44), ('islamic world', 40), ('islamic revolution', 32), ('ji sri', 28), ('imam khomeini', 28), ('years ago', 28), ('uttar pradesh', 24), ('free india', 24), ('come back', 24), ('tseh lee', 24), ('mr lien', 24), ('would like', 20), ('dear friends', 20), ('past years', 20), ('religious popularity', 20), ('palestinian state', 20), ('yuan tseh', 20), ('first time', 16), ('political parties', 16), ('government delhi', 16), ('five years', 16), ('become pm', 16), ('want tell', 16), ('want ask', 16), ('youth country', 16), ('thank god', 16), ('friends islamic', 16), ('interest rates', 16), ('four years', 16)]\n",
      "No texts for language de\n",
      "Top bigrams for fr: [('discours lalbisgüetli', 104), ('mesdames messieurs', 60), ('présidente confédération', 52), ('démocratie directe', 44), ('madame présidente', 36), ('conseil fédéral', 32), ('droits lhomme', 28), ('citoyennes citoyens', 20), ('libre circulation', 20), ('manière dutiliser', 16), ('droit international', 16), ('liebes volk', 16), ('cher peuple', 16), ('doivent être', 16), ('circulation personnes', 16), ('parti politique', 16), ('lalbisgüetli ludc', 12), ('palais fédéral', 12), ('droits humanitaires', 12), ('point vue', 12), ('droit libéral', 12), ('lalbisgüetli mesdames', 12), ('fédéral parlement', 12), ('citoyens non', 12), ('tribunal fédéral', 12), ('produit intérieur', 12), ('berne fédérale', 12), ('bon parti', 12), ('assemblée lalbisgüetli', 8), ('anc conseiller', 8)]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "df = pd.read_csv('..\\data\\processed\\gpd_processed.csv')\n",
    "\n",
    "# Berücksichtige nur Zeilen mit \"averagerubric\" größer als 1.5\n",
    "df = df.query(\"averagerubric > 1.5\")\n",
    "\n",
    "# Bereinigen Sie Ihre Daten\n",
    "df['cleaned_text'] = df['file_contents'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))  # Entfernen von Satzzeichen\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'\\d+', '', x))  # Entfernen von Zahlen\n",
    "\n",
    "# Füge eine neue Spalte hinzu, die die Sprache des Textes enthält\n",
    "df['language'] = df['cleaned_text'].apply(detect_language)\n",
    "\n",
    "# Zeige die Verteilung der Sprachen an\n",
    "print(df['language'].value_counts())\n",
    "\n",
    "# Bereinigen Sie Ihre Daten und entfernen Sie Stoppwörter, getrennt nach Sprachen\n",
    "stopwords_multilang = {\n",
    "    'en': set(stopwords.words('english')),\n",
    "    'de': set(stopwords.words('german')),\n",
    "    'fr': set(stopwords.words('french')),\n",
    "    # Fügen Sie hier weitere Sprachen hinzu, wenn nötig\n",
    "}\n",
    "\n",
    "def remove_stopwords(text, lang):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopwords_multilang[lang]])\n",
    "\n",
    "for lang in stopwords_multilang.keys():\n",
    "    df.loc[df['language'] == lang, 'cleaned_text'] = df.loc[df['language'] == lang, 'cleaned_text'].apply(lambda x: remove_stopwords(x, lang))\n",
    "\n",
    "\n",
    "def get_top_bigrams(texts, n=30):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "for lang in stopwords_multilang.keys():\n",
    "    texts = df.loc[df['language'] == lang, 'cleaned_text']\n",
    "    if texts.empty:\n",
    "        print(f\"No texts for language {lang}\")\n",
    "    else:\n",
    "        top_bigrams = get_top_bigrams(texts)\n",
    "        print(f'Top bigrams for {lang}: {top_bigrams}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f89325c4e608955f9112ed8991acc730d82c7a6d04377adb00f47e06b2b8546"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 ('facial_recognition_software-zpc5VR0q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
